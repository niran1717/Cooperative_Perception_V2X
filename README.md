
# Cooperative Perception for Autonomous Vehicles using LiDAR and LLM Integration

This project focuses on enhancing perception in autonomous vehicles through **cooperative LiDAR-based object detection** and introduces a future extension with **Large Language Models (LLMs)** for emergency decision-making. The implementation leverages the **TUMTraf V2X Cooperative Perception Dataset** and builds a LiDAR-only 3D object detection pipeline using a custom-built **PointPillar model**, trained and evaluated from scratch.

## ğŸš˜ Project Highlights

- **LiDAR-only 3D Object Detection** using PointPillar architecture
- Supports **Vehicle-only**, **Infrastructure-only**, and **Cooperative (Fused)** configurations
- Custom **data preprocessing** pipeline to handle the multi-agent TUMTraf dataset
- Bounding box **visualization in BEV (Birdâ€™s Eye View)** using both Matplotlib and Open3D

## ğŸ“ Folder Structure

```
â”œâ”€â”€ data_utils/             # All data processing and dataset utils
â”‚   â”œâ”€â”€ label_parser.py
â”‚   â”œâ”€â”€ loss.py
â”‚   â”œâ”€â”€ pointpillar.py
â”‚   â”œâ”€â”€ target_assigner.py
â”‚   â”œâ”€â”€ tumtraf_dataset.py
â”‚   â””â”€â”€ voxelizer.py
â”œâ”€â”€ datasets/               # Dataset descriptor files
â”‚   â””â”€â”€ dataset.txt
â”œâ”€â”€ models/                 # Model definition
â”‚   â””â”€â”€ model.py
â”œâ”€â”€ output/                 # Outputs (visualizations and evaluation)
â”‚   â”œâ”€â”€ LLM/                # Sample visuals from LLM integration
â”‚   â”œâ”€â”€ bev_comparisons/    # GT vs Pred visualizations
â”‚   â””â”€â”€ train_loss_curve.png
â”œâ”€â”€ preprocessing/          # Dataset preprocessing
â”‚   â””â”€â”€ preprocess.py
â”œâ”€â”€ training/               # Training scripts
â”‚   â””â”€â”€ lidar_train.py
â”œâ”€â”€ requirements.txt
```

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

# (Optional) Create virtual environment
python -m venv coopenv
source coopenv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Running the Project

### 1. Preprocess the Dataset

```bash
python preprocessing/preprocess.py
```

### 2. Train the LiDAR-Only PointPillar Model

```bash
python training/lidar_train.py
```

### 3. Visualize Predictions

```bash
# Output in output/bev_comparisons/
python visualize_bev.py
```

## ğŸ” LLM Module (Early Integration Phase)

An experimental **LLM-based emergency decision-making module** is prepared for future integration. It takes in cooperative perception scene descriptions and outputs risk assessments or recommended actions (e.g., "slow down", "brake").

This module is structured and located under `output/LLM/`, and will serve as a decision layer downstream of perception.

## ğŸ“ˆ Sample Results

- **Vehicle-only LiDAR**: 80.1 mAPâ‚ƒD Avg
- **Infra-only LiDAR**: 84.88 mAPâ‚ƒD Avg
- **Cooperative LiDAR**: **90.76 mAPâ‚ƒD Avg**

Visual samples are included in the repo under `output/bev_comparisons`.

## ğŸ“Œ Future Work

- Extend model to handle **camera-only** and **camera+LiDAR** setups
- Full integration with **LLM-based risk reasoning and planning**
- Explore deployment on CARLA for sim-to-real transfer
